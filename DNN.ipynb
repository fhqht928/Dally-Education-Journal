{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00f508b8",
   "metadata": {},
   "source": [
    "## Googlenet 영상인식 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "436dd3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opencv Deep learning Tutorial\n",
    "# https://github.com/opencv/opencv/wiki/Deep-Learning-in-OpenCV\n",
    "\n",
    "# Caffe Model Zoo : github.com/BVLC/caffe\n",
    "## 모델 파일 : dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel\n",
    "## 설정 파일 : github.com/BVLC/caffe/blob/master/models/bvlc_googlenet/deploy.prototxt\n",
    "\n",
    "\n",
    "# ONNX Model Zoo : github.com/onnx/models\n",
    "# 모델파일: https://github.com/onnx/models/tree/master/vision/classification/inception_and_googlenet/googlenet\n",
    "\n",
    "# 클래스 이름 파일 : github.com/opencv/opencv/blob/4.1.0/samples/data/dnn/\n",
    "\n",
    "# readNet(model, config)\n",
    "# model, config\n",
    "\n",
    "# 실행순서\n",
    "# cv2.dnn.readNet(model, config)-> ret, 객체생성\n",
    "# blobFromImage(image, scalefactor, size, mean, swapRB, crop) -> retval\n",
    "# scalefactor: Multiply by factor\n",
    "# image has BGR ordering and swapRB is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2983eed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1000)\n",
      "0.59207493\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "########### googLeNet 영상인식\n",
    "# 입력크기: 224 x 224\n",
    "# 컬러: BGR\n",
    "# 밝기평균값: (104, 117, 123)\n",
    "\n",
    "\n",
    "########## 입력 영상 불러오기\n",
    "\n",
    "# filename = 'googlenet/apple2.png'\n",
    "filename = './googlenet/fig/scooter.jpg'\n",
    "\n",
    "# if len(sys.argv) > 1: \n",
    "#     filename = sys.argv[1]\n",
    "\n",
    "img = cv2.imread(filename)\n",
    "\n",
    "if img is None:\n",
    "    print('Image load failed!')\n",
    "    sys.exit()\n",
    "\n",
    "######### 네트워크 불러오기\n",
    "\n",
    "# Caffe\n",
    "model = 'googlenet/bvlc_googlenet.caffemodel'\n",
    "config = 'googlenet/deploy.prototxt'\n",
    "\n",
    "# ONNX\n",
    "# model = 'googlenet/googlenet-9.onnx'\n",
    "# config = ''\n",
    "\n",
    "net = cv2.dnn.readNet(model, config)\n",
    "\n",
    "if net.empty():\n",
    "    print('Network load failed!')\n",
    "    sys.exit()\n",
    "\n",
    "########## 클래스 이름 불러오기\n",
    "\n",
    "classNames = []\n",
    "with open('googlenet/classification_classes_ILSVRC2012.txt', 'rt') as f:\n",
    "    classNames = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "\n",
    "########### 추론\n",
    "# blobFromImage(image[, scalefactor[, size[, mean[, swapRB[, crop[, ddepth]]]]]]) -> retval\n",
    "# retval: numpy.ndarry.shape = (N,C,H,W), dtype = numpy.float32\n",
    "## N = number of image, C = channels, H = height, W = width\n",
    "\n",
    "blob = cv2.dnn.blobFromImage(img, 1, (224, 224), (104, 117, 123), \n",
    "                             swapRB = False)\n",
    "net.setInput(blob)\n",
    "prob = net.forward()\n",
    "print(prob.shape)\n",
    "\n",
    "########### 추론 결과 확인 & 화면 출력\n",
    "\n",
    "out = prob.flatten() # 1d array\n",
    "classId = np.argmax(out)\n",
    "confidence = out[classId]\n",
    "print(confidence)\n",
    "text = f'{classNames[classId]} ({confidence * 100:4.2f}%)'\n",
    "cv2.putText(img, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e13fb3",
   "metadata": {},
   "source": [
    "## OpenCV DNN 얼굴검출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a40c568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/opencv/opencv/tree/master/samples/dnn/face_detector\n",
    "# deploy.prototxt.txt, download-weights.py.txt, opencv_face_detector.pbtxt.text 다운로드\n",
    "\n",
    "# Caffe    https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20180205_fp16/res10_300x300_ssd_iter_140000_fp16.caffemodel\n",
    "\n",
    "# Tensorflow  https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20180220_uint8/opencv_face_detector_uint8.pb\n",
    "\n",
    "## 참고 사이트\n",
    "# https://deep-learning-study.tistory.com/299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c56da7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('opencv_face_detector/fig/sunglass.png')\n",
    "\n",
    "if img is None:\n",
    "    print('image read failed')\n",
    "    sys.exit()\n",
    "    \n",
    "model = './opencv_face_detector/res10_300x300_ssd_iter_140000_fp16.caffemodel'\n",
    "config = './opencv_face_detector/deploy.prototxt'\n",
    "\n",
    "face_net = cv2.dnn.readNet(model, config)\n",
    "\n",
    "if face_net.empty():\n",
    "    print('Net open failed')\n",
    "    sys.exit()\n",
    "\n",
    "blob = cv2.dnn.blobFromImage(img, 1, (300, 300), (104, 177, 123),\n",
    "                            swapRB=False)\n",
    "\n",
    "face_net.setInput(blob)\n",
    "out = face_net.forward()\n",
    "\n",
    "detect = out[0, 0, :, :]\n",
    "h, w = img.shape[:2]\n",
    "\n",
    "for i in range(detect.shape[0]):\n",
    "    confidence = detect[i, 2]\n",
    "    \n",
    "    if confidence > 0.5:\n",
    "        # out matrix에서 x1, y1, x2, y2 값이 0 ~1로 normalize 되어 있음\n",
    "        \n",
    "        x1 = int(detect[i, 3]*w)\n",
    "        y1 = int(detect[i, 4]*h)\n",
    "        x2 = int(detect[i, 5]*w)\n",
    "        y2 = int(detect[i, 6]*h)\n",
    "        \n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2),\n",
    "                     (0, 0, 255))\n",
    "        \n",
    "        text = 'Face: {}%'.format(round(confidence, 2))\n",
    "        cv2.putText(img, text, (x1, y1-1), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                   0.8, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "cv2.imshow('image', img)\n",
    "\n",
    "while True:\n",
    "    if cv2.waitKey() == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cba0da2",
   "metadata": {},
   "source": [
    "## YOLOv3를 이용한 객체 검출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e8ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pjreddie.com/darknet/yolo/\n",
    "\n",
    "# NMSBoxes(bboxes, scores, score_threshold, nms_threshold) -> indices\n",
    "# nms_threshold: nms_threshold a threshold used in non maximum suppression\n",
    "\n",
    "# getPerfProfile() -> retval, timings\n",
    "# .   @brief Returns overall time for inference and timings (in ticks) for layers.\n",
    "\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "493eb7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "# 모델 & 설정 파일\n",
    "model = 'yolo_v3/yolov3.weights'\n",
    "config = 'yolo_v3/yolov3.cfg'\n",
    "class_labels = 'yolo_v3/coco.names'\n",
    "\n",
    "confThreshold = 0.5\n",
    "nmsThreshold = 0.4\n",
    "\n",
    "# 테스트 이미지 파일\n",
    "img_files = ['yolo_v3/dog.jpg', 'yolo_v3/person.jpg', \n",
    "             'yolo_v3/sheep.jpg', 'yolo_v3/kite.jpg']\n",
    "\n",
    "# 네트워크 생성\n",
    "net = cv2.dnn.readNet(model, config)\n",
    "\n",
    "if net.empty():\n",
    "    print('Net open failed!')\n",
    "    sys.exit()\n",
    "\n",
    "# 클래스 이름 불러오기\n",
    "\n",
    "classes = []\n",
    "with open(class_labels, 'rt') as f:\n",
    "    classes = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "# 출력 레이어 이름 받아오기\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "# output_layers = ['yolo_82', 'yolo_94', 'yolo_106']\n",
    "\n",
    "# 실행\n",
    "\n",
    "for i in img_files:\n",
    "    img = cv2.imread(i)\n",
    "\n",
    "    if img is None:\n",
    "        continue\n",
    "\n",
    "    # 블롭 생성 & 추론\n",
    "    blob = cv2.dnn.blobFromImage(img, 1/255., (320, 320), swapRB=True)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers) #\n",
    "\n",
    "    # outs는 3개의 ndarray 리스트.\n",
    "    # outs[0].shape=(507, 85), 13*13*3=507\n",
    "    # outs[1].shape=(2028, 85), 26*26*3=2028\n",
    "    # outs[2].shape=(8112, 85), 52*52*3=8112\n",
    "\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            # detection: 4(bounding box) + 1(objectness_score) + 80(class confidence)\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > confThreshold:\n",
    "                # 바운딩 박스 중심 좌표 & 박스 크기\n",
    "                cx = int(detection[0] * w)\n",
    "                cy = int(detection[1] * h)\n",
    "                bw = int(detection[2] * w)\n",
    "                bh = int(detection[3] * h)\n",
    "\n",
    "                # 바운딩 박스 좌상단 좌표\n",
    "                sx = int(cx - bw / 2)\n",
    "                sy = int(cy - bh / 2)\n",
    "\n",
    "                boxes.append([sx, sy, bw, bh])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(int(class_id))\n",
    "\n",
    "    # 비최대 억제, Non Max Suppression\n",
    "#     https://www.visiongeek.io/2018/07/yolo-object-detection-opencv-python.html\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, confThreshold, nmsThreshold)\n",
    "\n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        sx, sy, bw, bh = boxes[i]\n",
    "        label = f'{classes[class_ids[i]]}: {confidences[i]:.2}'\n",
    "        color = colors[class_ids[i]]\n",
    "        cv2.rectangle(img, (sx, sy, bw, bh), color, 2)\n",
    "        cv2.putText(img, label, (sx, sy - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2, cv2.LINE_AA)\n",
    "\n",
    "    t, _ = net.getPerfProfile()\n",
    "    label = 'Inference time: %.2f ms' % (t * 1000.0 / cv2.getTickFrequency())\n",
    "    cv2.putText(img, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.7, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('img', img)\n",
    "    cv2.waitKey()\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ae63b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
